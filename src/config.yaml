num_classes: 77
num_workers: 1

batch_size: 8
max_num_epochs: 50

transforms:
  normalize:
    mean: [0.9044, 0.9044, 0.9044] #[0.485, 0.456, 0.406] # #tensor([0.9074, 0.9074, 0.9074])
    std:  [0.1485, 0.1485, 0.1485] #tensor([0.1318, 0.1318, 0.1318]) [0.229, 0.224, 0.225] #
                                                          
model:
  name: vit-mae-large #densenet169 #swin #vit # deit
  pretrained: true
  fine_tune: true  # If true, fine-tune the entire model; if false, freeze earlier layers
  freeze_until: none  # Options could be "all", "none", or specific layer like "layer4"

# relevant for deit
distilled: true

optimizer:
  type: "Adam"  # or "SGD" or "AdamW", or "MSProp" ?
  lr: 0.00005

scheduler:
  enable: false
  type: "CosineAnnealingLR"
  T_max: 50  # Number of epochs before first restart
  eta_min: 0.000001  # Minimum lr

early_stopping:
  early_stopping_metric: balanced_accuracy # loss, accuracy or balanced_accuracy
  patience: 5

reports:
  enable: true
  frequency: 1

#augmentations:


checkpoint: true # save model and training parameters

dataset: ZooScan77_small